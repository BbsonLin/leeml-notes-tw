# 梯度下降(Gradient descent, GD)

<!-- 在第二篇文章中有介绍到梯度下降法的做法，传送门：机器学习入门系列02，Regression 回归：案例研究 -->

## 回顧: 梯度下降法

回想一下我們在 [Chapter3 寶可夢 CP 值預估範例中的 Step3. Best Function]()，  
我們曾用梯度下降來找到損失函數(Loss function) 的最低值。

我們可以歸納出一個通用的方程式:

$\theta^∗ = arg \min_{\theta} L(\theta^∗)$

> * $L$: Loss function (損失函數)
> * $\theta$: Parameters (参數)

> 注意: 参數($\theta^*$) 是複數的，例如 Chapter3 的 $w$ 和 $b$。

--

今天我們想要找一組參數 $\theta$，讓損失函數越小越好，這問題可用梯度下降法來解決。

![](res/chapter6-1.png)

假設用 $\theta^*$ 中的兩個參數 $\theta_1, \theta_2$，隨機選取初始值:

$
\theta^0 = \begin{bmatrix}
\theta_1^0 \\
\theta_2^0
\end{bmatrix}
$

然後分別計算初始點處，兩個參數對 $L$ 的偏微分，最後 $\theta^0$ 減掉 $\eta$ 乘上偏微分的值，更新得到下一組參數；  
反復進行這樣的計算，這就是梯度下降法。

> $
> \nabla L(\theta) = \begin{bmatrix}
> \cfrac {\partial L(\theta_1)}{\partial \theta_1} \\
> \cfrac {\partial L(\theta_2)}{\partial \theta_2}
> \end{bmatrix}
> $
> 為更簡潔的寫法(即為一個向量[vector])，$\nabla L(\theta)$ 即為梯度。
>
> $\eta$: Learning rates(學習率 eta)

將上述過程視覺化，如下圖。

![](res/chapter6-2.png)

---

## 梯度下降法的小撇步

### Tip 1：調整學習率，小心地調整學習率(Learning rate)

有時候如何設定學習率(Learning rate)，會造成我們一些的困擾。

舉例:

![](res/chapter6-3.png)

上面左圖中，黑色的為損失函數的曲線；

* 紅色，表示為剛剛好的學習率，可以順利地走到最低點。
* 藍色，學習率偏小，速度太慢，需要花很多時間才能走到最低點。
* 綠色，學習率偏大，更新幅度太大，有可能永遠走到最低點。
* 黃色，學習率太大，可能會發生越更新，損失函數越大的情況。

雖然左圖中，可以很直覺觀察，但這種視覺化只有參數是一維或二維能夠這樣做，更高維的情況就無法視覺化了。

解決方式是如右圖，我們永遠可以視覺化參數更新時對損失函數的變化；所以在作梯度下降的時候，最好把這類圖畫出來，可以更早的發現學習率是否太大或太小。

#### 自適應的學習率

那有沒有方式讓學習率(Learning rate)自己作調整?

我們可以作簡單的構想：隨著更新參數次數的增加，學習率應該要越小:

* 通常剛開始，初始點會距離最低點比較遠，所以會希望使用大一點的學習率，能快一點到最低點。
* 更新好幾次參數之後，比較靠近最低點了，因此減少學習率，讓損失函數能收斂在最低點的地方。
* 比如 $\eta^t = \cfrac{\eta^t}{\sqrt{t+1}}$，$t$ 是次數。隨著次數的增加，$\eta^t$ 減小

--

但是光以上這樣是不夠的，有時學習率不會是一個值通用所有參數(特徵)，所以最好的狀況是，不同的參數給不同的學習率。

#### Adagrad

一般的的梯度下降可以表示為：

$w^{t+1} \leftarrow w^t -η^tg^t$

$\eta^t = \cfrac{\eta^t}{\sqrt{t+1}}$

> $w$ 為其中一個參數
> $g$ 為梯度(gradient)，也就是前一小節的 $\nabla L(\theta)$
> $t$ 為更新的次數

Adagrad 可以做的比一般梯度下降更：

$w^{t+1} \leftarrow  w^t -\cfrac{η^t}{\sigma^t}g^t$

$g^t =\cfrac{\partial L(\theta^t)}{\partial w}$

> $\sigma^t$: 過去所有算過微分值的均方根(root mean square)，因此對於每個參數都是不一樣的。

#### Adagrad 講解

下圖是 Adagrad 參數的更新過程

![](res/chapter6-4.png)


將以上 Adagrad 的過程進行化簡：

![](res/chapter6-5.png)

因為 $\eta^t$ 和 $\sigma^t$ 中都包含 $\sqrt{t+1}$，可以消除。


#### Adagrad 存在的矛盾?

![](res/chapter6-6.png)

在我們推出來的式子中，梯度 $g$ 越大，更新的步伐就越大；但分母中的 $\sqrt{\Sigma^t_{i=0}(g^i)^2}$ 反而會因為 $g$ 越小，更新的步伐就越小。

有點衝突，如何解釋這件事情呢?

--

##### 直觀的解釋：

![](res/chapter6-7.png)

為了強調梯度 $g$ 在更新參數中的 **反差**；因為 $\sqrt{\sum^t_{i=0}(g^i)^2}$ 把過去的 $g$ 都考慮進來。

--

##### 正式的解釋：

1. 二次函數

    ![](res/chapter6-8.png)

    上圖為 $y=ax^2+bx+c$；下圖為上圖的微分 $|\cfrac{\partial y}{\partial x}|=|2ax+b|$。

    初始點在$x_0$，最低點為$−\frac{b}{2a}$，那麼最佳的步伐就是$x_0$ 到最低點之間的距離$| x_0+\frac{b}{2a} |$，也可以寫成$\cfrac{|2ax_0+b|}{2a}$。而分子 $|2ax_0+b|$ 就是二次函數在 $x_0$ 這一點的微分。

    這樣可以認為如果算出來的微分越大，則距離最低點越遠。而且最好的步伐和微分的大小成正比。所以如果踏出去的步伐和微分成正比，它可能是比較好的。

    **結論a**：梯度越大，就跟最低點的距離越遠。

    但這個結論在多個參數的時候就不一定成立了

2. 多個參數的函數

    ![](res/chapter6-9.png)

    左圖是兩個參數($w_1$, $w_2$)的損失函數，顏色代表損失函數的值。
    
    如果只考慮參數 $w_1$，在左圖中藍色的水平線切一刀，得到右圖上面結果；如果只考慮參數 $w_2$，在左圖中綠色的垂直線切一刀，得到右圖下面的結果。
    
    確實對於 $a$ 和 $b$，**結論a** 是成立的，同理對於 $c$ 和 $d$ 也成立；不過對比 $a$ 和 $c$，就不成立了，$c$ 比 $a$ 大，而 $c$ 距離最低點卻是比較近的。

    所以 **結論a** 是在沒有考慮跨參數對比的情況下，才能成立的。所以還不完善。

3. 考慮二次微分

    之前說到二次函數中的最佳距離 $\cfrac{|2ax_0+b|}{2a}$，還有個分母 $2a$。  
    我們再對二次函數進行二次微分剛好可以得到：$\cfrac{\partial ^2y}{\partial x^2} = 2a$  
    所以最好的步伐應該是：$\cfrac{|一次微分|}{二次微分}$

    意即不止和一次微分成正比，還和二次微分成反比。

    ![](res/chapter6-10.png)

##### 更進一步的解釋:

再回到之前的 Adagrad

![](res/chapter6-11.png)

分母項並不是二次微分阿!?

在實際情況中，如果計算二次微分非常有可能會增加時間消耗；所以使用 $\sqrt{\sum^t_{i=0}(g^i)^2}$ 就是希望在盡可能不增加過多運算的情況下模擬二次微分。

> $\sqrt{\sum^t_{i=0}(g^i)^2}$ 就是在一次微分後的函數上取樣(sampling)，再作平方和取開根號，


### Tip 2：隨機梯度下降(Stochastic Gradient Descent, SGD)

一般的梯度下降：

$L=\sum_n(\hat y^n-(b+\sum w_ix_i^n))^2$ [Loss function]

$\theta^i=\theta^{i-1}-\eta\nabla L(\theta^{i-1})$

隨機梯度下降：

損失函數一次只考慮其中一筆資料 $x^n$

$L=(\hat y^n-(b+\sum w_ix_i^n))^2$ [Loss function only for one example]

$\theta^i =\theta^{i-1}-\eta\nabla L^n(\theta^{i-1})$

SGD 中不需要像之前那樣對所有的數據進行處理，只需要計算某一筆資料的損失函數Ln，就可以趕緊更新參數。

對比：

![](res/chapter6-12.png)

GD 走一步要處理到所有二十個例子，但 SGD 此時已經走了二十步（每處理一筆資料就更新）


### Tip3：特徵縮放(Feature Scaling)

比如有个函数：

$y=b+w_1x_1+w_2x_2$

針對兩個不同參數資料的分佈範圍很不一樣，建​​議把他們的範圍縮放，使得不同資料的分佈範圍是一樣的。

![](res/chapter6-13.png)


#### 為什麼要做特徵縮放？

![](res/chapter6-14.png)

左圖是 $x_1$ 的 scale 比 $x_2$ 要小很多，所以當 $w_1$ 和 $w_2$ 做同樣的變化時候，$x_1$ 對 $y$ 的變化影響是比較小的，$x_2$ 對 $y$ 的變化影響是比較大。

左圖座標系是還沒有做過 scaling 的 ，從 [Chapter 4](/chapter4/chapter4)) 中我們可以看到，像這種橢圓形 error surface 做一般的梯度下降是有難度的，如果不用分別參數的 learning rate (Adagrad) 是很難只用一組 learning rate 搞定他的。

右圖是做過 scaling，右圖座標系的 error surface 就比較接近圓形，再做梯度下降參數更新時會比較有效率。


#### 怎麼做特徵縮放？

方法非常多種，這裡舉例一種常見的做法：

![](res/chapter6-15.png)

上圖每一列都是一個維度($i$)，裡面都有一組特徵。

算出每一維度的算術平均($m_i$)與標準差($\sigma_i$)。

然後每個資料 $x^r_i$ 都縮放成 $\cfrac{x^r_i - m_i}{\sigma_i}$，這樣就可以保證每一維度的資料都能保持
算術平均是 0 且分布在 -1 ~ 1 的區間內(變異數是 1)。


## 梯度下降的理論基礎

先問個問題:

> 當用梯度下降：
> $\theta^∗= arg \max_{\theta} L(\theta)$
> 每次更新參數 $\theta$，都得到一個新的 $\theta$，它都使得損失函數更小。即：
> $L(\theta^0) >L(\theta^1)>L(\theta^2)>···$

上述結論正確嗎？ 答案是不正確的

--

### 形式推導

假如要解一個最佳解的問題(假設不知道 GD)，在下圖上找最低點，應該怎麼想呢?

![](res/chapter6-16.png)

等高線是 $L(\theta)$ 損失函數，並給初始點 $\theta^0$。

在 $\theta^0$ 处，我們可以在一个小范围(紅圈)内找到损失函数最小的 $\theta^1$，不断更新參數並寻找。

--

接下来就是如何在紅圈内快速的找到最小值？


### 泰勒級數(Taylor Series)

先介紹一下泰勒級數

#### 定義

若 $h(x)$ 在 $x=x_0$ 點的某個領域內有無限階導數（即無限可微分，infinitely differentiable），那麼在此領域內有：

$
\begin{aligned}
h(x) &= \sum_{k=0}^{\infty }\frac{h^k(x_0)}{k!}(x-x_0)^k \\
&= h(x_0)+{h}'(x_0)(x−x_0)+\frac{h''(x_0)}{2!}(x−x_0)^2+⋯
\end{aligned}
$

後者為 $x=x_0$ 的泰勒展開式(Taylor Expansion)。

而當 $x$ 很接近 $x_0$ 時，可簡化泰勒展開式為 $h(x)≈h(x_0)+{h}'(x_0)(x−x_0)$

舉例：

![](res/chapter6-17.png)

圖中三條藍色線是把前三次項作圖，橙色線是 $sin(x)$。

如果我們看一次式(也就是斜直的藍線)，會發現完全跟 $sin(x)$(橙線)不像阿；但只有在 $x$ 接近 $\cfrac {\pi}4$ 時所有線最接近同一個值。

#### 多變量泰勒展開式

下面是兩個變量的泰勒展開式

![](res/chapter6-18.png)

#### 利用泰勒展開式簡化

回到之前如何快速在圓圈內找到最小值。基於泰勒展開式，在 $(a,b)$ 點的紅色圓圈範圍內，可以將損失函數 $L(\theta)$ 用泰勒展開式進行簡化：

![](res/chapter6-19.png)

將問題進而簡化為下圖：

![](res/chapter6-20.png)

設紅圈的半徑為 $d$，紅圈所有的點可表示成 $(\theta_1-a)^2+(\theta_2-b)^2 \leq d^2$

![](res/chapter6-21.png)

尋找能讓 $L(\theta)$ 最小的向量$(\Delta \theta_1,\Delta \theta_2)$ 時不用考慮 $s$(與 $\theta$ 沒關係，可忽略)，接下來可以看出剩下的部分就是 $(\Delta \theta_1,\Delta \theta_2)$ 和$(u,v)$ 的內積。

那怎樣讓它最小? 就是和向量$(u,v)$ 方向相反的向量。

![](res/chapter6-22.png)

然後將 $u$ 和 $v$ 帶入，可得到

$
\begin{bmatrix}
\theta_1 \\
\theta_2
\end{bmatrix} =
\begin{bmatrix}
a \\
b
\end{bmatrix} - \eta
\begin{bmatrix}
u \\
v
\end{bmatrix} =
\begin{bmatrix}
a \\
b
\end{bmatrix} - \eta
\begin{bmatrix}
\cfrac {\partial L(a,b)}{\partial \theta_1} \\
\cfrac {\partial L(a,b)}{\partial \theta_2}
\end{bmatrix}
$

這式子其實就是梯度下降(Gradient Descent)。

但此等式要成立，必須要建立在 

$$L(\theta)\approx s+u(\theta_1 - a)+v(\theta_2 - b) \tag1$$ 上，

而這個損失函數的接近值，就必須是之前泰勒展開式所提的 $x$ 要非常接近 $x_0$ 時才行[也就是 (\theta_1,\theta_2) 要非常接近 $(a,b)$]；因此我們紅圈的半徑為 $d$，要非常的小；這也是為什麼梯度下降時學習率 $\eta$ 通常都很小。

所以當更新參數的時候，如果學習率沒有設好，是有可能式子 $(1)$ 是不成立的，所以導致做梯度下降的時候，損失函數沒有越來越小。

> 式子$(1)$ 只考慮了泰勒展開式的一次項，如果考慮到二次項（比如牛頓法），在實際中不是特別好，會涉及到二次微分等，多很多的運算，性價比不好。


## 梯度下降的限制

![](res/chapter6-23.png)

* 容易陷入局部極值。(local minimun)
* 可能卡在不是極值，但微分值是0的地方。(saddle point)
* 可能實際中只是當微分值小於某一個數值就停下來了，但這裡只是比較平緩，並不是極值點。


## 怎麼會不知道極值的位置呢?

### 世紀帝國比喻

真正在做梯度下降(GD)時，可想像成是玩世紀帝國一樣。

- 在地圖上大多數位置我們是未知的，只有我們單位走過的地方是可知。
- 地圖上的海拔可以看作損失函數**loss function**，我們的目的就是尋找海拔的最低點的值；
- 隨機初始一個位置，朝向較低的方向移動，周而復始，直到**local minimun**(在不開天眼的情況下，你始終不會知曉所在位置是否為global minimun)。


## 過程中損失函數不減反增?

### Minecraft 比喻

- 人物的前方是較低方向，右方也是較低方向，利用梯度下降法，往右前方移動一步，然後反復用梯度下降法，往右前方移動一步，周而復始；
- 遇到前方和右方是下降的方向，儘管右前方是比較高的地方，還是會更新到此處。
