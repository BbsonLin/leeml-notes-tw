# 梯度下降(Gradient descent, GD)

<!-- 在第二篇文章中有介绍到梯度下降法的做法，传送门：机器学习入门系列02，Regression 回归：案例研究 -->

## 回顧: 梯度下降法

回想一下我們在 [Chapter3 寶可夢 CP 值預估範例中的 Step3. Best Function]()，  
我們曾用梯度下降來找到損失函數(Loss function) 的最低值。

我們可以歸納出一個通用的方程式:

$\theta^∗ = arg \min_{\theta} L(\theta^∗)$

> * $L$: Loss function (損失函數)
> * $\theta$: Parameters (参數)

> 注意: 参數($\theta^*$) 是複數的，例如 Chapter3 的 $w$ 和 $b$。

--

今天我們想要找一組參數 $\theta$，讓損失函數越小越好，這問題可用梯度下降法來解決。

![](res/chapter6-1.png)

假設用 $\theta^*$ 中的兩個參數 $\theta_1, \theta_2$，隨機選取初始值:

$
\theta^0 = \begin{bmatrix}
\theta_1^0 \\
\theta_2^0
\end{bmatrix}
$

然後分別計算初始點處，兩個參數對 $L$ 的偏微分，最後 $\theta^0$ 減掉 $\eta$ 乘上偏微分的值，更新得到下一組參數；  
反復進行這樣的計算，這就是梯度下降法。

> $
> \nabla L(\theta) = \begin{bmatrix}
> \cfrac {\partial L(\theta_1)}{\partial \theta_1} \\
> \cfrac {\partial L(\theta_2)}{\partial \theta_2}
> \end{bmatrix}
> $
> 為更簡潔的寫法(即為一個向量[vector])，$\nabla L(\theta)$ 即為梯度。
>
> $\eta$: Learning rates(學習率 eta)

將上述過程視覺化，如下圖。

![](res/chapter6-2.png)

---

## 梯度下降法的小撇步

### Tip 1：調整學習率，小心地調整學習率(Learning rate)

有時候如何設定學習率(Learning rate)，會造成我們一些的困擾。

舉例:

![](res/chapter6-3.png)

上面左圖中，黑色的為損失函數的曲線；

* 紅色，表示為剛剛好的學習率，可以順利地走到最低點。
* 藍色，學習率偏小，速度太慢，需要花很多時間才能走到最低點。
* 綠色，學習率偏大，更新幅度太大，有可能永遠走到最低點。
* 黃色，學習率太大，可能會發生越更新，損失函數越大的情況。

雖然左圖中，可以很直覺觀察，但這種視覺化只有參數是一維或二維能夠這樣做，更高維的情況就無法視覺化了。

解決方式是如右圖，我們永遠可以視覺化參數更新時對損失函數的變化；所以在作梯度下降的時候，最好把這類圖畫出來，可以更早的發現學習率是否太大或太小。

#### 自適應的學習率

那有沒有方式讓學習率(Learning rate)自己作調整?

我們可以作簡單的構想：隨著更新參數次數的增加，學習率應該要越小:

* 通常剛開始，初始點會距離最低點比較遠，所以會希望使用大一點的學習率，能快一點到最低點。
* 更新好幾次參數之後，比較靠近最低點了，因此減少學習率，讓損失函數能收斂在最低點的地方。
* 比如 $\eta^t = \cfrac{\eta^t}{\sqrt{t+1}}$，$t$ 是次數。隨著次數的增加，$\eta^t$ 減小

--

但是光以上這樣是不夠的，有時學習率不會是一個值通用所有參數(特徵)，所以最好的狀況是，不同的參數給不同的學習率。

#### Adagrad

一般的的梯度下降可以表示為：

$w^{t+1} \leftarrow w^t -η^tg^t$

$\eta^t = \cfrac{\eta^t}{\sqrt{t+1}}$

> 這裡的 $w$ 是一個參數

Adagrad 可以做的比一般梯度下降更：

$w^{t+1} \leftarrow  w^t -\cfrac{η^t}{\sigma^t}g^t$

$g^t =\cfrac{\partial L(\theta^t)}{\partial w}$

> $\sigma^t$: 過去所有算過微分值的均方根(root mean square)，因此對於每個參數都是不一樣的。

#### Adagrad举例
下图是一个参数的更新过程

![](res/chapter6-4.png)

将 Adagrad 的式子进行化简：
![](res/chapter6-5.png)


#### Adagrad 存在的矛盾？
![](res/chapter6-6.png)

在 Adagrad 中，当梯度越大的时候，步伐应该越大，但下面分母又导致当梯度越大的时候，步伐会越小。

下图是一个直观的解释：

![](res/chapter6-7.png)

下面给一个正式的解释：

![](res/chapter6-8.png)

比如初始点在 $x_0$，最低点为 $−\frac{b}{2a}$，最佳的步伐就是 $x0$ 到最低点之间的距离 $\left | x_0+\frac{b}{2a} \right |$，也可以写成 $\left | \frac{2ax_0+b}{2a} \right |$。而刚好 $|2ax_0+b|$ 就是方程绝对值在 $x_0$ 这一点的微分。

这样可以认为如果算出来的微分越大，则距离最低点越远。而且最好的步伐和微分的大小成正比。所以如果踏出去的步伐和微分成正比，它可能是比较好的。

结论1-1：梯度越大，就跟最低点的距离越远。

这个结论在多个参数的时候就不一定成立了。

#### 多参数下结论不一定成立
对比不同的参数

![](res/chapter6-9.png)

上图左边是两个参数的损失函数，颜色代表损失函数的值。如果只考虑参数 $w_1$，就像图中蓝色的线，得到右边上图结果；如果只考虑参数 $w_2$，就像图中绿色的线，得到右边下图的结果。确实对于 $a$ 和 $b$，结论1-1是成立的，同理 $c$ 和 $b$ 也成立。但是如果对比$a$ 和 $c$，就不成立了，$c$ 比 $a$ 大，但 $c$ 距离最低点是比较近的。

所以结论1-1是在没有考虑跨参数对比的情况下，才能成立的。所以还不完善。

之前说到的最佳距离 $\left | \frac{2ax_0+b}{2a} \right |$，还有个分母 $2a$ 。对function进行二次微分刚好可以得到：
$$\frac{\partial ^2y}{\partial x^2} = 2a \tag7$$
所以最好的步伐应该是：
$$\frac{一次微分}{二次微分}$$
即不止和一次微分成正比，还和二次微分成反比。最好的step应该考虑到二次微分：

![](res/chapter6-10.png)

#### Adagrad 进一步的解释
再回到之前的 Adagrad

![](res/chapter6-11.png)

对于 $\sqrt{\sum_{i=0}^t(g^i)^2}$ 就是希望再尽可能不增加过多运算的情况下模拟二次微分。（如果计算二次微分，在实际情况中可能会增加很多的时间消耗）

## Tip2：随机梯度下降法

之前的梯度下降：

$$L=\sum_n(\hat y^n-(b+\sum w_ix_i^n))^2 \tag8$$
$$\theta^i =\theta^{i-1}- \eta\triangledown L(\theta^{i-1}) \tag9$$

而随机梯度下降法更快：

损失函数不需要处理训练集所有的数据，选取一个例子 $x^n$

$$L=(\hat y^n-(b+\sum w_ix_i^n))^2 \tag{10}$$
$$\theta^i =\theta^{i-1}- \eta\triangledown L^n(\theta^{i-1}) \tag{11}$$

此时不需要像之前那样对所有的数据进行处理，只需要计算某一个例子的损失函数Ln，就可以赶紧update 梯度。

对比：

![](res/chapter6-12.png)

常规梯度下降法走一步要处理到所有二十个例子，但随机算法此时已经走了二十步（每处理一个例子就更新）


## Tip3：特征缩放
比如有个函数：

$$y=b+w_1x_1+w_2x_2 \tag{12}$$
两个输入的分布的范围很不一样，建议把他们的范围缩放，使得不同输入的范围是一样的。

![](res/chapter6-13.png)

### 为什么要这样做？
![](res/chapter6-14.png)

上图左边是 $x_1$ 的scale比 $x_2$ 要小很多，所以当 $w_1$ 和 $w_2$ 做同样的变化时，$w_1$ 对 $y$ 的变化影响是比较小的，$x_2$ 对 $y$ 的变化影响是比较大的。

坐标系中是两个参数的error surface（现在考虑左边蓝色），因为 $w_1$ 对 $y$ 的变化影响比较小，所以 $w_1$ 对损失函数的影响比较小，$w_1$ 对损失函数有比较小的微分，所以 $w_1$ 方向上是比较平滑的。同理 $x_2$ 对 $y$ 的影响比较大，所以 $x_2$ 对损失函数的影响比较大，所以在 $x_2$ 方向有比较尖的峡谷。

上图右边是两个参数scaling比较接近，右边的绿色图就比较接近圆形。

对于左边的情况，上面讲过这种狭长的情形不过不用Adagrad的话是比较难处理的，两个方向上需要不同的学习率，同一组学习率会搞不定它。而右边情形更新参数就会变得比较容易。左边的梯度下降并不是向着最低点方向走的，而是顺着等高线切线法线方向走的。但绿色就可以向着圆心（最低点）走，这样做参数更新也是比较有效率。


### 怎么做缩放？

方法非常多，这里举例一种常见的做法：

![](res/chapter6-15.png)

上图每一列都是一个例子，里面都有一组特征。

对每一个维度 $i$（绿色框）都计算平均数，记做 $m_i$；还要计算标准差，记做 $\sigma _i$。

然后用第 $r$ 个例子中的第 $i$ 个输入，减掉平均数 $m_i$，然后除以标准差 $\sigma _i$，得到的结果是所有的维数都是 $0$，所有的方差都是 $1$

## 梯度下降的理论基础
### 问题
当用梯度下降解决问题：

$$\theta^∗= \underset{ \theta }{\operatorname{arg\ max}}  L(\theta) \tag1$$

每次更新参数 $\theta$，都得到一个新的 $\theta$，它都使得损失函数更小。即：

$$L(\theta^0) >L(\theta^1)>L(\theta^2)>···\tag{13}$$

上述结论正确吗？

结论是不正确的。。。

## 数学理论
![](res/chapter6-16.png)

比如在 $\theta^0$ 处，可以在一个小范围的圆圈内找到损失函数细小的 $\theta^1$，不断的这样去寻找。

接下来就是如果在小圆圈内快速的找到最小值？


### 泰勒展开式

先介绍一下泰勒展开式

#### 定义
若 $h(x)$ 在 $x=x_0$ 点的某个领域内有无限阶导数（即无限可微分，infinitely differentiable），那么在此领域内有：

$$
\begin{aligned}
h(x)  &= \sum_{k=0}^{\infty }\frac{h^k(x_0)}{k!}(x-x_0)^k  \\
& =h(x_0)+{h}'(x_0)(x−x_0)+\frac{h''(x_0)}{2!}(x−x_0)^2+⋯
\tag{14}
\end{aligned} 
$$


当 $x$ 很接近 $x_0$ 时，有 $h(x)≈h(x_0)+{h}'(x_0)(x−x_0)$
式14 就是函数 $h(x)$ 在 $x=x_0$ 点附近关于 $x$ 的幂函数展开式，也叫泰勒展开式。

举例：

![](res/chapter6-17.png)

图中3条蓝色线是把前3项作图，橙色线是 $sin(x)$。

#### 多变量泰勒展开式
下面是两个变量的泰勒展开式

![](res/chapter6-18.png)

### 利用泰勒展开式简化
回到之前如何快速在圆圈内找到最小值。基于泰勒展开式，在 $(a,b)$ 点的红色圆圈范围内，可以将损失函数用泰勒展开式进行简化：

![](res/chapter6-19.png)

将问题进而简化为下图：

![](res/chapter6-20.png)

不考虑s的话，可以看出剩下的部分就是两个向量$(\triangle \theta_1,\triangle \theta_2)$ 和  $(u,v)$ 的内积，那怎样让它最小，就是和向量 $(u,v)$ 方向相反的向量

![](res/chapter6-21.png)

然后将u和v带入。

![](res/chapter6-22.png)
$$L(\theta)\approx s+u(\theta_1 - a)+v(\theta_2 - b) \tag{14}$$

发现最后的式子就是梯度下降的式子。但这里用这种方法找到这个式子有个前提，泰勒展开式给的损失函数的估算值是要足够精确的，而这需要红色的圈圈足够小（也就是学习率足够小）来保证。所以理论上每次更新参数都想要损失函数减小的话，即保证式1-2 成立的话，就需要学习率足够足够小才可以。

所以实际中，当更新参数的时候，如果学习率没有设好，是有可能式1-2是不成立的，所以导致做梯度下降的时候，损失函数没有越来越小。

式1-2只考虑了泰勒展开式的一次项，如果考虑到二次项（比如牛顿法），在实际中不是特别好，会涉及到二次微分等，多很多的运算，性价比不好。

## 梯度下降的限制
![](res/chapter6-23.png)

容易陷入局部极值
还有可能卡在不是极值，但微分值是0的地方
还有可能实际中只是当微分值小于某一个数值就停下来了，但这里只是比较平缓，并不是极值点


